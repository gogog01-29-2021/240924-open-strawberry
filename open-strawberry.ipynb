{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWkkuulhyyx_"
   },
   "source": [
    "# open-strawberry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fGumS80I_8jL"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U typing-extensions\n",
    "!pip install anthropic openai google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "GIqhT8GXDyd3"
   },
   "outputs": [],
   "source": [
    "# https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\n",
    "import anthropic\n",
    "clawd_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "clawd_client = anthropic.Anthropic(api_key=clawd_key) if clawd_key else None\n",
    "\n",
    "def get_anthropic(model: str, prompt: str, temperature: float = 0, system: str = ''):\n",
    "    message = clawd_client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=1024,\n",
    "        temperature=temperature,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "          {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        system=system,\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "AONfP2RkK4qY"
   },
   "outputs": [],
   "source": [
    "# Also applies to ollama, vLLM, h2oGPT, etc.\n",
    "from openai import OpenAI\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "openai_client = OpenAI(api_key=openai_key) if openai_key else None\n",
    "\n",
    "def get_openai(model: str, prompt: str, temperature: float = 0, system: str = ''):\n",
    "    messages = [{'role': 'system', 'content': system},\n",
    "                {'role': 'user', 'content': prompt}]\n",
    "    responses = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/google-gemini/cookbook/\n",
    "# https://ai.google.dev/gemini-api/docs/caching?lang=python\n",
    "import google.generativeai as genai\n",
    "\n",
    "gemini_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=gemini_key) if gemini_key else None\n",
    "\n",
    "def get_gemini(model: str, prompt: str, temperature: float = 0, system: str = ''):\n",
    "    model = genai.GenerativeModel(model, system_instruction=system, generation_config={'temperature': temperature})\n",
    "    chat = model.start_chat(history=[])\n",
    "    response = chat.send_message(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Let us play a game of \"take only the most minuscule step toward the solution.\"\n",
    "<thinking_game>\n",
    "* The assistant's text output must be only the very next possible step.\n",
    "* Use your text output as a scratch pad in addition to a literal output of some next step.\n",
    "* Everytime you make a major shift in thinking, output your high-level current thiking in <thinking> </thinking> XML tags.\n",
    "* You should present your response in a way that iterates on that scratch pad space with surrounding textual context.\n",
    "* You win the game is you are able to take the smallest text steps possible while still (on average) heading towards the solution.\n",
    "* Backtracking is allowed, and generating python code is allowed (but will not be executed, but can be used to think), just on average over many text output turns you must head towards the answer.\n",
    "* You should think like a human, and ensure you identify inconsistencies, errors, etc.\n",
    "</thinking_game>\n",
    "Are you ready to win the game?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
